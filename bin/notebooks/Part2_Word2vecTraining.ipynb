{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Tuning, Training and Saving\n",
    "\n",
    "In this notebook, we will tune the Word2Vec model to be as performant as possible. We will then train it and finally save it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading of the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved FastText model...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from models.word2vec import Word2Vec\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils.utils import word2vecFineTuning, DataSamplization\n",
    "\n",
    "dataSamplization = DataSamplization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetching of all the relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching of the skipgram pairs from data/skipgramPairs/word_pairs_fromWikiDump.txt\n",
    "skipGramWordIDPairs = []\n",
    "with open('data/skipgramPairs/word_pairs_fromWikiDump.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        skipGramWordIDPairs.append(line.strip().split())\n",
    "skipGramWordIDPairs = [(int(target), int(context)) for target, context in skipGramWordIDPairs]\n",
    "\n",
    "# Fetching of the 30-dim pre-trained embeddings for fine-tuning\n",
    "embeddings = np.array(np.load('data/modelsSavedLocally/wikipedia/30dim_embeddings_ArraySimple.npy'))\n",
    "embeddings = torch.tensor(embeddings)\n",
    "vocab_size = len(embeddings)\n",
    "embedding_dim = 30\n",
    "\n",
    "skipgram_data = torch.tensor(skipGramWordIDPairs, dtype=torch.long)\n",
    "dataset = TensorDataset(skipgram_data[:, 0], skipgram_data[:, 1])\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hdrrayan/Documents/COURS/GALATASARAY/Artificial Neural Networks/French-reducted-word-embedding/French-reducted-word-embedding/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 62.6818\n",
      "Epoch [25/500], Loss: 18.6224\n",
      "Epoch [50/500], Loss: 8.4228\n",
      "Epoch [75/500], Loss: 5.2409\n",
      "Epoch [100/500], Loss: 4.0894\n",
      "Epoch [125/500], Loss: 3.5895\n",
      "Epoch [150/500], Loss: 3.3594\n",
      "Epoch [175/500], Loss: 3.2479\n",
      "Epoch [200/500], Loss: 3.1940\n",
      "Epoch [225/500], Loss: 3.1655\n",
      "Epoch [250/500], Loss: 3.1509\n",
      "Epoch [275/500], Loss: 3.1409\n",
      "Epoch [300/500], Loss: 3.1353\n",
      "Epoch [325/500], Loss: 3.1309\n",
      "Epoch [350/500], Loss: 3.1034\n",
      "Epoch [375/500], Loss: 3.0795\n",
      "Epoch [400/500], Loss: 3.0769\n",
      "Epoch [425/500], Loss: 3.0761\n",
      "Early stopping triggered at epoch 425.\n",
      "Fine-tuning completed.\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(vocab_size, embedding_dim, embeddings)\n",
    "model = word2vecFineTuning(model, dataloader, epochs=500, lr=0.001)\n",
    "\n",
    "fine_tuned_embeddings = model.target_embeddings.weight.data.numpy()\n",
    "dic_before_tuning_embeds_30dim = np.load(allow_pickle=True, file=\"data/modelsSavedLocally/wikipedia/30dim_embeddings_DictWithWords.npy\").item()\n",
    "dic_fine_tuned_embeds_30dim = {word : fine_tuned_embeddings[i] for i, word in enumerate(dic_before_tuning_embeds_30dim.keys())}\n",
    "np.save('data/modelsSavedLocally/wikipedia/Tuned30dim_embeddings_DictWithWords.npy', dic_fine_tuned_embeds_30dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we loaded 30-dimensional embeddings, trained them with Wikipedia-extracted French data using a basic word2vec architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
