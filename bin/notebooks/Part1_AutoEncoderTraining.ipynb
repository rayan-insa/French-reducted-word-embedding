{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder Hyperparameter Tuning\n",
    "\n",
    "This notebook documents the steps for experimenting with and selecting hyperparameters for training the AutoEncoder. We will load the data, define hyperparameters, train the model, evaluate performance, and visualize results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Required Libraries\n",
    "\n",
    "We will start by loading the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")  # Move up one directory\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from models.autoencoder import AutoEncoder\n",
    "from utils.utils import DataSamplization, autoEncoderTraining, dataBatching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Next, we will load the sample data that will be used for training and testing the AutoEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "sampleNum = 1000\n",
    "dataSamplization = DataSamplization(sampleNum=sampleNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take data from random words of the vocabulary if prefered\n",
    "data = dataSamplization.randomWordsDataSample(sampleNum=sampleNum)\n",
    "print([data[10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take data from most-used words if prefered\n",
    "data = dataSamplization.mostUsedDataSample(sampleNum=sampleNum)\n",
    "print([data[10:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Training\n",
    "\n",
    "We will preprocess the data and create DataLoader objects for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Normalization and fitting to pytorch of the embedding matrix, then splitting and batching\n",
    "embedding_matrix = np.array([dataSamplization.fastTextBaseModel.get_word_vector(word) for word in data])\n",
    "scaler = StandardScaler()\n",
    "embedding_matrix_normalized = scaler.fit_transform(embedding_matrix)\n",
    "train_data, test_data = train_test_split(embedding_matrix_normalized, test_size=0.2, random_state=42)\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "\n",
    "print(f\"Train dataset shape : {train_tensor.shape}\")\n",
    "print(f\"Test dataset shape : {test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the AutoEncoder\n",
    "\n",
    "Now we will train the AutoEncoder using the defined hyperparameters. The goal is to find the best configuration possible for our AutoEncoder. We will do the following : Hyperparameters definition, Training of all combinations, Evaluate each bottleneck output against the base model, Visualize evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. HyperParameters definition\n",
    "\n",
    "Quick loop to define all the possible configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 300\n",
    "bottleneck_dim = 30\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "min_delta = 0.001\n",
    "\n",
    "# Hyperparameters\n",
    "hp_dic = {'hidden_dim1':[256, 128], 'hidden_dim2':[128, 64], 'learning_rate':[0.01, 0.001, 0.0001], 'batch_size':[32, 64]}\n",
    "\n",
    "hyperparameter_grid = []\n",
    "for hidden_dim1 in hp_dic['hidden_dim1']:\n",
    "    for hidden_dim2 in hp_dic['hidden_dim2']:\n",
    "        if hidden_dim1 <= hidden_dim2:\n",
    "            continue\n",
    "        for learning_rate in hp_dic['learning_rate']:\n",
    "            for batch_size in hp_dic['batch_size']:\n",
    "                hyperparameter_grid.append({'hidden_dim1':hidden_dim1, 'hidden_dim2':hidden_dim2, 'learning_rate':learning_rate, 'batch_size':batch_size})\n",
    "\n",
    "print(hyperparameter_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training and cosine-similarity evaluation of each configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Results tracker\n",
    "results = []\n",
    "\n",
    "# Hyperparameter tuning loop\n",
    "for idx, config in enumerate(hyperparameter_grid):\n",
    "    print(f\"\\nTesting configuration {idx + 1}: {config}\\n\")\n",
    "    train_dataloader, test_dataloader = dataBatching(\n",
    "        train_tensor, test_tensor, batch_size=config['batch_size']\n",
    "    )\n",
    "    autoEncoder = AutoEncoder(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim1=config['hidden_dim1'],\n",
    "        hidden_dim2=config['hidden_dim2'],\n",
    "        bottleneck_dim=bottleneck_dim\n",
    "    )\n",
    "    \n",
    "    # Train the AutoEncoder\n",
    "    train_losses, test_losses, bottleneck_outputs = autoEncoderTraining(\n",
    "        num_epochs=num_epochs,\n",
    "        patience=patience,\n",
    "        min_delta=min_delta,\n",
    "        lr=config['learning_rate'],\n",
    "        autoEncoder=autoEncoder,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader\n",
    "    )\n",
    "    \n",
    "    # Evaluate bottleneck (compressed) embeddings\n",
    "    reconstructed_embeddings = autoEncoder.decoder(bottleneck_outputs).detach().numpy()\n",
    "\n",
    "    similarity = np.mean([\n",
    "        np.dot(reconstructed_embeddings[i], embedding_matrix_normalized[i]) /\n",
    "        (np.linalg.norm(reconstructed_embeddings[i]) * np.linalg.norm(embedding_matrix_normalized[i]))\n",
    "        for i in range(len(reconstructed_embeddings))\n",
    "    ])\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'config': config,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_test_loss': test_losses[-1],\n",
    "        'cosine_similarity': similarity\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Taking out the worst configurations, keeping the best 3 in terms of test loss and the best 3 in terms of cosine similarity\n",
    "\n",
    "results = sorted(results, key=lambda x: x['final_test_loss'])\n",
    "best_test_loss = results[:3]\n",
    "results = sorted(results, key=lambda x: x['cosine_similarity'], reverse=True)\n",
    "best_cosine_similarity = results[:3]\n",
    "\n",
    "best_results = best_test_loss + best_cosine_similarity\n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "results = best_results\n",
    "labels = [f\"Config {i+1}\" for i in range(len(results))]\n",
    "train_losses = [res['final_train_loss'] for res in results]\n",
    "test_losses = [res['final_test_loss'] for res in results]\n",
    "similarities = [res['cosine_similarity'] for res in results]\n",
    "\n",
    "x = np.arange(len(labels))  # Label locations\n",
    "width = 0.3  # Bar width\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6), constrained_layout=True)\n",
    "\n",
    "# Bar plot for Losses\n",
    "ax[0].bar(x - width/2, train_losses, width, label='Train Loss', color='skyblue')\n",
    "ax[0].bar(x + width/2, test_losses, width, label='Test Loss', color='salmon')\n",
    "ax[0].set_title('Loss Comparison')\n",
    "ax[0].set_xlabel('Configurations')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xticks(x)\n",
    "ax[0].set_xticklabels(labels)\n",
    "ax[0].legend()\n",
    "\n",
    "# Bar plot for Cosine Similarity\n",
    "ax[1].bar(x, similarities, width, label='Cosine Similarity', color='limegreen')\n",
    "ax[1].set_title('Cosine Similarity Comparison')\n",
    "ax[1].set_xlabel('Configurations')\n",
    "ax[1].set_ylabel('Cosine Similarity')\n",
    "ax[1].set_xticks(x)\n",
    "ax[1].set_xticklabels(labels)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.suptitle('Hyperparameter Evaluation Metrics', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Find the best configuration\n",
    "best_config = min(results, key=lambda x: x['final_test_loss'])\n",
    "second_best_config = min(results, key=lambda x: x['cosine_similarity'])\n",
    "print(f\"Best configuration in terms of test loss : {best_config}\")\n",
    "print(f\"Best configuration in terms of cosine similarity : {second_best_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe very close cosine similarity for the two best results : ~0.020 and ~0.019 => 0.001 of difference\n",
    "We also observe very close test loss for both : ~0.567 and ~0.585 => 0.018 of difference.\n",
    "\n",
    "A factor 10 between both differences, hence we will choose the test loss as our criteria.\n",
    "\n",
    "#### Finally, the best configuration is : hidden_dim1 = 256, hidden_dim2 = 128, learning_rate = 0.001, batch_size = 64\n",
    "\n",
    "Another way to confirm that it is the best configuration, is to not define any seed for the data split and the batching, in order to leave some randomness between different trainings, and then execute the loop that comes before many times to check which configuration is consistantly in the best two. And it is indeed the 'hidden_dim1 = 256, hidden_dim2 = 128, learning_rate = 0.001, batch_size = 64' one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we documented the steps for hyperparameter tuning of the AutoEncoder.\n",
    "\n",
    "Thanks to this, we know that the best configuration to train our AutoEncoder with is the following :\n",
    "\n",
    "### hidden_dim1 = 256, hidden_dim2 = 128, learning_rate = 0.001, batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final training of the AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training with the best configuration and more epochs\n",
    "\n",
    "# Load the dataset\n",
    "data = dataSamplization.randomWordsDataSample(sampleNum=100000)\n",
    "\n",
    "# Normalization and fitting to pytorch of the embedding matrix, then splitting and batching\n",
    "embedding_matrix = np.array([dataSamplization.fastTextBaseModel.get_word_vector(word) for word in data])\n",
    "scaler = StandardScaler()\n",
    "embedding_matrix_normalized = scaler.fit_transform(embedding_matrix)\n",
    "train_data, test_data = train_test_split(embedding_matrix_normalized, test_size=0.2, random_state=42)\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "\n",
    "autoEncoder = AutoEncoder(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim1=best_config['config']['hidden_dim1'],\n",
    "    hidden_dim2=best_config['config']['hidden_dim2'],\n",
    "    bottleneck_dim=bottleneck_dim\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader = dataBatching(\n",
    "    train_tensor, test_tensor, batch_size=best_config['config']['batch_size']\n",
    ")\n",
    "\n",
    "train_losses, test_losses, bottleneck_outputs = autoEncoderTraining(\n",
    "    num_epochs=2000,\n",
    "    patience=patience,\n",
    "    min_delta=min_delta,\n",
    "    lr=best_config['config']['learning_rate'],\n",
    "    autoEncoder=autoEncoder,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader\n",
    ")\n",
    "\n",
    "\n",
    "# plot the training and test losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss', color='skyblue')\n",
    "plt.plot(test_losses, label='Test Loss', color='salmon')\n",
    "plt.title('Train and Test Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save the model\n",
    "os.makedirs(\"data/modelsSavedLocally/\", exist_ok=True)\n",
    "torch.save(autoEncoder.state_dict(), 'data/modelsSavedLocally/autoencoder.pth')\n",
    "print(\"Model saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
